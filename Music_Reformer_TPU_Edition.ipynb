{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "[STABLE]Music_Reformer_TPU_Edition.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asigalov61/Music-Reformer/blob/main/Music_Reformer_TPU_Edition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgn4KUAATtXS"
      },
      "source": [
        "# Music Reformer (v.1.0): TPU Edition\r\n",
        "\r\n",
        "### This is a work in progress so please check back for updates and improvements.\r\n",
        "\r\n",
        "***\r\n",
        "\r\n",
        "### Based on the offical Reformer Google Colab and code.\r\n",
        "\r\n",
        "***\r\n",
        "\r\n",
        "Project Los Angeles\r\n",
        "\r\n",
        "Tegridy Code 2021\r\n",
        "\r\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxJaH9ga8bQr"
      },
      "source": [
        "# Setup the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QINqPZH6FzB3",
        "cellView": "form"
      },
      "source": [
        "#@title Install the dependencies\n",
        "# Install dependencies\n",
        "\n",
        "!git clone https://github.com/asigalov61/tegridy-tools\n",
        "%cd /content/tegridy-tools/tegridy-tools/\n",
        "%cd /content/\n",
        "\n",
        "!wget https://github.com/asigalov61/Music-Reformer/raw/main/Dataset/Music-Reformer_TXT_Dataset.zip\n",
        "!unzip Music-Reformer_TXT_Dataset.zip\n",
        "\n",
        "!pip install --upgrade -q jax\n",
        "!pip install --upgrade -q jaxlib\n",
        "!pip install --upgrade -q trax==1.3.6\n",
        "!pip install --upgrade -q sentencepiece\n",
        "!pip install --upgrade -q gin \n",
        "\n",
        "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
        "import requests\n",
        "import os\n",
        "if 'TPU_DRIVER_MODE' not in globals():\n",
        "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
        "  resp = requests.post(url)\n",
        "  TPU_DRIVER_MODE = 1\n",
        "\n",
        "# The following is required to use TPU Driver as JAX's backend.\n",
        "from jax.config import config\n",
        "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
        "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
        "print(config.FLAGS.jax_backend_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv0PuxC3F-jn",
        "cellView": "form"
      },
      "source": [
        "#@title Import modules\n",
        "import gin\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from scipy.special import softmax\n",
        "\n",
        "%cd /content/tegridy-tools/tegridy-tools\n",
        "import TMIDI\n",
        "%cd /content/\n",
        "\n",
        "\n",
        "# Zipping and downloading files\n",
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Trax\n",
        "import jax\n",
        "import trax\n",
        "from trax.data import inputs\n",
        "import jax.numpy as jnp\n",
        "\n",
        "# NLP Vocab Generation\n",
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7OJzWNu8YG9"
      },
      "source": [
        "# Prep the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7shBqm0T6rz",
        "cellView": "form"
      },
      "source": [
        "#@title Process the TXT MIDI dataset to plain TXT data\n",
        "with open('/content/Music-Reformer_TXT_Dataset.txt', 'rb') as file:\n",
        "  z = file.read().split()\n",
        "  out = []\n",
        "  for i in range(len(z)):\n",
        "    try:\n",
        "      out.append(str(z[i].decode('utf-8')))\n",
        "    except:\n",
        "      continue\n",
        "\n",
        "out1 = '\\n'.join(out)\n",
        "\n",
        "X = np.fromstring(out1, dtype='int8')\n",
        "output = ''\n",
        "\n",
        "for y in X:\n",
        "  try:\n",
        "    output += str(abs(y)) + '\\n'\n",
        "  except:\n",
        "    continue\n",
        "\n",
        "with open('/content/Music-Reformer_INT_Dataset.txt', 'w') as f:\n",
        "  f.write(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l9Znw2bS-Pg",
        "cellView": "form"
      },
      "source": [
        "#@title Create a tokenizer and its model\n",
        "# Train a BPE model on the dataset\n",
        "spm.SentencePieceTrainer.train('--input=/content/Music-Reformer_TXT_Dataset.txt \\\n",
        "                                --model_prefix=Music-Reformer-Tokenizer \\\n",
        "                                --vocab_size=3600 \\\n",
        "                                --model_type=bpe')\n",
        "# Load BPE vocabulary\n",
        "TOKENIZER = spm.SentencePieceProcessor() \n",
        "TOKENIZER.load('Music-Reformer-Tokenizer.model')\n",
        "\n",
        "# Load the dataset\n",
        "with open('/content/Music-Reformer_TXT_Dataset.txt') as f:\n",
        "    text = f.read()\n",
        "\n",
        "\n",
        "IDS = TOKENIZER.EncodeAsIds(text[:1100000])\n",
        "IDS = np.asarray(IDS, dtype=np.int32)\n",
        "PAD_AMOUNT = 512 * 1024 - len(IDS)\n",
        "print(\"Number of tokens:\", IDS.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQwhEqJzS-I2",
        "cellView": "form"
      },
      "source": [
        "#@title Split the dataset\n",
        "\n",
        "# Tokenize (set to max for the provided dataset)\n",
        "trX, vaX = np.split(X, [1100000])\n",
        "data_train, data_val = torch.from_numpy(trX), torch.from_numpy(vaX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glkNjfVX8TKL"
      },
      "source": [
        "# Setup the Reformer model and functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdKnx2FyS-AO",
        "cellView": "form"
      },
      "source": [
        "#@title Initialize the functions and procedures for training\n",
        "# Set up the data pipeline.\n",
        "def my_inputs(n_devices):\n",
        "  while True:\n",
        "    inputs = []\n",
        "    mask = []\n",
        "    pad_amounts = np.random.choice(PAD_AMOUNT, n_devices)\n",
        "    for i in range(n_devices):\n",
        "      inputs.append(np.pad(IDS, (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]), # Pad IDS by different amount for each device\n",
        "                            mode='constant'))\n",
        "      mask.append(np.pad(np.ones_like(IDS, dtype=np.float32),\n",
        "                          (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
        "                          mode='constant'))\n",
        "    inputs = np.stack(inputs)\n",
        "    mask = np.stack(mask)\n",
        "    yield (inputs, inputs, mask)\n",
        "\n",
        "print(\"(device count, tokens per device) = \",\n",
        "      next(my_inputs(trax.fastmath.device_count()))[0].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NeFGlGGS959",
        "cellView": "form"
      },
      "source": [
        "#@title Configure hyperparamenters\n",
        "# Configure hyperparameters.\n",
        "gin.parse_config(\"\"\"\n",
        "import trax.layers\n",
        "import trax.models\n",
        "import trax.optimizers\n",
        "import trax.data.inputs\n",
        "import trax.supervised.trainer_lib\n",
        "\n",
        "# Parameters that will vary between experiments:\n",
        "# ==============================================================================\n",
        "train.model = @trax.models.ReformerLM\n",
        "# Model will have 6 layers, alternating between the LSH attention\n",
        "# and local attention within a certain context window.\n",
        "n_layers = 6\n",
        "attn_type = [\n",
        "  @trax.layers.SelfAttention,\n",
        "  @LSHSelfAttention,  \n",
        "  @trax.layers.SelfAttention,\n",
        "  @LSHSelfAttention,\n",
        "  @trax.layers.SelfAttention,\n",
        "  @LSHSelfAttention,\n",
        "  ]\n",
        "share_qk = False  # LSH attention ignores this flag and always shares q & k\n",
        "n_heads = 2\n",
        "attn_kv = 64\n",
        "dropout = 0.05\n",
        "n_tokens = 524288\n",
        "\n",
        "# Parameters for multifactor:\n",
        "# ==============================================================================\n",
        "multifactor.constant = 0.01\n",
        "multifactor.factors = 'constant * linear_warmup * cosine_decay'\n",
        "multifactor.warmup_steps = 100\n",
        "multifactor.steps_per_cycle = 900\n",
        "\n",
        "# Parameters for Adam:\n",
        "# ==============================================================================\n",
        "Adam.weight_decay_rate=0.0\n",
        "Adam.b1 = 0.86\n",
        "Adam.b2 = 0.92\n",
        "Adam.eps = 1e-9\n",
        "\n",
        "# Parameters for SelfAttention:\n",
        "# ==============================================================================\n",
        "trax.layers.SelfAttention.attention_dropout = 0.05\n",
        "trax.layers.SelfAttention.chunk_len = 64\n",
        "trax.layers.SelfAttention.n_chunks_before = 1\n",
        "trax.layers.SelfAttention.n_parallel_heads = 1\n",
        "\n",
        "# Parameters for LSHSelfAttention:\n",
        "# ==============================================================================\n",
        "LSHSelfAttention.attention_dropout = 0.0\n",
        "LSHSelfAttention.chunk_len = 64\n",
        "LSHSelfAttention.n_buckets = [64, 128]\n",
        "LSHSelfAttention.n_chunks_after = 0\n",
        "LSHSelfAttention.n_chunks_before = 1\n",
        "LSHSelfAttention.n_hashes = 1\n",
        "LSHSelfAttention.n_parallel_heads = 1\n",
        "LSHSelfAttention.predict_drop_len = 128\n",
        "LSHSelfAttention.predict_mem_len = 1024\n",
        "\n",
        "# Parameters for ReformerLM:\n",
        "# ==============================================================================\n",
        "ReformerLM.attention_type = %attn_type\n",
        "ReformerLM.d_attention_key = %attn_kv\n",
        "ReformerLM.d_attention_value = %attn_kv\n",
        "ReformerLM.d_model = 256\n",
        "ReformerLM.d_ff = 512\n",
        "ReformerLM.dropout = %dropout\n",
        "ReformerLM.ff_activation = @trax.layers.Relu\n",
        "ReformerLM.max_len = %n_tokens\n",
        "ReformerLM.mode = 'train'\n",
        "ReformerLM.n_heads = %n_heads\n",
        "ReformerLM.n_layers = %n_layers\n",
        "ReformerLM.vocab_size = 320\n",
        "ReformerLM.axial_pos_shape = (512, 1024)\n",
        "ReformerLM.d_axial_pos_embs= (64, 192)\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8ay-6VxS9xw",
        "cellView": "form"
      },
      "source": [
        "#@title Setup the model and the trainer routines\n",
        "# Trainer.\n",
        "output_dir = os.path.expanduser('model')\n",
        "!rm -f ~/model/model.pkl.gz  # Remove old model\n",
        "\n",
        "trainer = trax.supervised.Trainer(\n",
        "    model=trax.models.ReformerLM,\n",
        "    loss_fn=trax.layers.CrossEntropyLoss(),\n",
        "    optimizer=trax.optimizers.Adam,\n",
        "    lr_schedule=trax.lr.multifactor(),\n",
        "    inputs=trax.data.inputs.Inputs(my_inputs),\n",
        "    output_dir=output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeMJKx6P8Puc"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf0HITY7Wz8C",
        "cellView": "form"
      },
      "source": [
        "#@title Train the model\n",
        "# Train Model\n",
        "import tqdm\n",
        "for _ in tqdm.tqdm(range(50)):\n",
        "  trainer.train_epoch(n_steps=100, n_eval_steps=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEikLctyte-m",
        "cellView": "form"
      },
      "source": [
        "#@title Zip and download your trained model checkpoint here\n",
        "# Zip directory contents\n",
        "shutil.make_archive(\"project\", \"zip\", \".\")\n",
        "\n",
        "# Download zipped directory\n",
        "files.download('project.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1vc_Pa_8JGj"
      },
      "source": [
        "# Generate Music"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLebeiAFrItK",
        "cellView": "form"
      },
      "source": [
        "#@title Increase hashing rounds number for better quality here\n",
        "# In the Reformer paper, increasing the number of hashing rounds helps with quality. \n",
        "# The number of hashing rounds at can be increased at evaluation time only.\n",
        "gin.parse_config(\"\"\"LSHSelfAttention.n_hashes = 4\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "cquc4SJD0Y_J"
      },
      "source": [
        "#@title Load the trained Reformer in 'predict' mode\n",
        "# Load the trained Reformer in 'predict' mode\n",
        "model = trax.models.ReformerLM(mode='predict')\n",
        "output_dir = os.path.expanduser('model')\n",
        "model.init_from_file(os.path.join(output_dir,'model.pkl.gz'),\n",
        "                     weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thdiRvGDrJhy",
        "cellView": "form"
      },
      "source": [
        "#@title Generate and decode music from the model\n",
        "# Sample from ReformerLM\n",
        "output_token_ids = trax.supervised.decoding.autoregressive_sample(\n",
        "    model, temperature=0.8, max_length=1024, batch_size = 4)\n",
        "\n",
        "# Decode token IDs\n",
        "# Reformer outputed a batch with one item so access it using [0]\n",
        "# tolist() converts from int64 to int, the type SentencePiece expects\n",
        "input = TOKENIZER.DecodeIds(output_token_ids[0].tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox03iX3KHCKa",
        "cellView": "form"
      },
      "source": [
        "#@title Convert generated output to MIDI.\n",
        "# Run the cells below to convert generated output to MIDI.\n",
        "# If you getting errors/halts, regenerate the output again.\n",
        "# Model must be sufficiently trained. Rec. 0.90+ accuracy for the output to make sense and pass error control.\n",
        "\n",
        "TXT = TMIDI.Tegridy_INT_String_to_TXT_Converter(input, line_by_line_input=False)\n",
        "SONG = TMIDI.Tegridy_Reduced_TXT_to_Notes_Converter(TXT, has_MIDI_channels=False, has_velocities=False, dataset_includes_beat=True)\n",
        "stats = TMIDI.Tegridy_SONG_to_MIDI_Converter(SONG=SONG[0], output_file_name='/content/Music-Reformer_MIDI')\n",
        "print(stats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7RAK2XE8jdE"
      },
      "source": [
        "# Congrats!!! You did it!!! :)"
      ]
    }
  ]
}